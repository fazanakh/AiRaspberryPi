{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_face(image_path, output_size=(224, 224)):\n",
    "    \n",
    "    # Detects faces in an image and crops the image to only include the face.\n",
    "    # The cropped face is resized to `output_size`.\n",
    "    \n",
    "    # Parameters:\n",
    "    # - image_path: Path to the input image.\n",
    "    # - output_size: Desired size of the output image.\n",
    "    \n",
    "    # Returns:\n",
    "    # - Cropped and resized face image as a PIL Image object. Returns None if no face is detected.\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        return None  # No faces detected\n",
    "    \n",
    "    # For simplicity, only consider the first detected face\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = img[y:y+h, x:x+w]\n",
    "        face = cv2.resize(face, output_size)\n",
    "        face_pil = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
    "        return face_pil\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading + Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"C:\\Users\\faiza\\Documents\\Uni\\Year 3\\Final Project\\Code\\images\"\n",
    "#The below is for the cropped images to be stored at\n",
    "cropped_faces_dir = r\"C:\\Users\\faiza\\Documents\\Uni\\Year 3\\Final Project\\Code\\faces\"\n",
    "if not os.path.exists(cropped_faces_dir):\n",
    "    os.makedirs(cropped_faces_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(cropped_faces_dir):\n",
    "    pass\n",
    "else:\n",
    "    # Assume images are organized in subdirectories for each class\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        \n",
    "        save_dir = os.path.join(cropped_faces_dir, class_name)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        for img_file in glob.glob(class_dir + '/*.jpg'):  # Adjust the pattern as needed\n",
    "            img_name = os.path.basename(img_file)\n",
    "            save_path = os.path.join(save_dir, img_name)\n",
    "            \n",
    "            cropped_face = crop_face(img_file)\n",
    "            if cropped_face is not None:\n",
    "                cropped_face.save(save_path)\n",
    "            else:\n",
    "                img_file.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(cropped_faces_dir).with_suffix('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angry = list(data_dir.glob('angry/*'))\n",
    "PIL.Image.open(str(angry[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(str(angry[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy = list(data_dir.glob('happy/*'))\n",
    "PIL.Image.open(str(happy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(str(happy[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size refers to the number of training examples utilised in one iteration. It will process 32 images at a time during training\n",
    "batch_size = 32\n",
    "#the height of the images will be restricted to 180\n",
    "img_height = 180\n",
    "#the width of the images will be restricted to 180\n",
    "img_width = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begins the creation of the training dataset using tensorflow's keras api\n",
    "#image_dataset_from_directory is used to load and preprocess the images so that it is suitable to train a model\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2, #80% of the data will be trained and 20% will be used to test it\n",
    "    subset=\"training\", #this specifies that it is for training\n",
    "    seed=123, #random number generator to shuffle the dataset and create the split\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begins the creation of the validation dataset using tensorflow's keras api\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\", #this specifies that it is for validation\n",
    "    seed=123, #this is the same to keep the split the same\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the class names\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this optimises the performance of the training and validation datasets\n",
    "#tf.data.AUTOTUNE prompts tensorflow to tune the buffering of elements in the data pipeline for optimal performance\n",
    "#by setting it as a variable, it is being prepared to be used in the dataset operations that follow, allowing TensorFlow\n",
    "#to dynamically adjust how much data to prefetch based on the current system's available resources\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "#caches elements of train_ds. this loads images and labels into memory during the first epoch (iteration over dataset)\n",
    "#this ensures them being reused in in subsequent epochs, speeding the training as I/O is reduced\n",
    "#prefetch(buffer_size=AUTOTUNE) - prefetches the data; this is whilst the model is executing step 's' the input pipeline\n",
    "#is reading step 's+1'\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a normalisation layer 1/255 is the factor each pixel will be multiplied. converts [0,255] to [0,1]\n",
    "normalization_layer = layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maps the values [0,255] to [0,1]. that only applies to the x values and not the y values\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "#Squential is a linear stack of layers which is one of the most simplest types of models in Keras. layers are just\n",
    "#stacked in sequence\n",
    "model = Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)), #the first layer; reschales layer to normalise the pixel values of the input image\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'), #a 2D convolutional layer with 16 filters of size 3x3; RELU activation + padding='same'\n",
    "    #means the output size will be the same as the input\n",
    "    layers.MaxPooling2D(), #a max pooling layer reduces spatial dimensions of the input volume; helps with reducing the no. of parameters and computation\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'), #the depth of filters is increased\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(), #flattens the 3D output into a 1D array. this is because it will be used as the input to dense layers (which expect 1D arrays)\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adam optimiser used. it is a popular choice for optimisation\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=40\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applies random transformations to the data. This improves the generalisation of the model and reduces overfitting\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\", #flips horizontally (left to right)\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3)),\n",
    "    layers.RandomRotation(0.1), #randomly rotates the images by a random degree within the range [-10% * 2π, 10% * 2π]\n",
    "    layers.RandomZoom(0.1), #it randomly zooms the images in and out by 10%\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    augmented_images = data_augmentation(images)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  data_augmentation, # will apply what happened in data augmentation from before\n",
    "  layers.Rescaling(1./255), #normalises data after augmentation. scales the pixels to [0, 1]\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'), #convolutional and pooling layers are added for feature extraction\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2), #a dropout layer is added which converts 2D feature maps to a 1D feature vector, allowing it to be fed into the dense layers\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes, name=\"outputs\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load the image\n",
    "pic_path = r\"C:\\Users\\faiza\\Downloads\\1700725738962.jpeg\"\n",
    "img = cv2.imread(pic_path)\n",
    "\n",
    "# Ensure the image was loaded\n",
    "if img is None:\n",
    "    raise ValueError(\"Failed to load image\")\n",
    "\n",
    "# Resize the image for faster face detection\n",
    "img = cv2.resize(img, (640, int(img.shape[0] * (640/img.shape[1]))))\n",
    "# Convert to grayscale for face detection\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "if len(faces) > 0:\n",
    "    # Crop to the first detected face\n",
    "    (x, y, w, h) = faces[0]\n",
    "    face_img = img[y:y+h, x:x+w]\n",
    "\n",
    "    # Display the cropped face image\n",
    "    cv2.imshow('Cropped Face', face_img)\n",
    "    cv2.waitKey(0)  # Wait for a key press to continue\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Optional: Save the cropped face image to a file\n",
    "    cropped_face_path = r\"C:\\Users\\faiza\\Downloads\\cropped_face.jpg\"\n",
    "    cv2.imwrite(cropped_face_path, face_img)\n",
    "\n",
    "    # Prepare the cropped face image for model prediction\n",
    "    face_img = cv2.resize(face_img, (img_height, img_width))\n",
    "    face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB) / 255.0  # Normalize\n",
    "    img_array = np.expand_dims(face_img, axis=0)\n",
    "else:\n",
    "    print(\"No face detected, using the whole image instead.\")\n",
    "    img = tf.keras.preprocessing.image.load_img(pic_path, target_size=(img_height, img_width))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "test_img_array = np.random.rand(1, img_height, img_width, 3).astype(np.float32)  # Example random input\n",
    "predictions = model.predict(test_img_array)  # Test prediction\n",
    "\n",
    "# Predict with the model\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "predicted_class = class_names[np.argmax(score)]\n",
    "confidence = 100 * np.max(score)\n",
    "\n",
    "print(f\"This image most likely belongs to {predicted_class} with a {confidence:.2f} percent confidence.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = tf.nn.softmax(predictions[0])\n",
    "for i, prob in enumerate(probabilities):\n",
    "    print(f\"{class_names[i]}: {prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Keras Sequential Model to Tensorflow Lite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_MODEL_FILE_PATH = 'model.tflite' # The default path to the saved TensorFlow Lite model\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=TF_MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = input_details[0]['shape']\n",
    "input_dtype = input_details[0]['dtype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"C:\\Users\\faiza\\Downloads\\WhatsApp Image 2024-02-15 at 10.53.10_5587b68b1.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(image_path).resize((input_shape[1], input_shape[2]))\n",
    "img_array = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = img_array / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if img_array.ndim == 2:\n",
    "    img_array = np.stack((img_array,) * 3, axis=-1)\n",
    "elif img_array.shape[2] == 4:\n",
    "    img_array = img_array[..., :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.expand_dims(img_array, axis=0).astype(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = np.argmax(output_data, axis=1)\n",
    "print(f\"Predicted Class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming output_data is the output from your TFLite model\n",
    "score_lite = tf.nn.softmax(output_data).numpy()\n",
    "score_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score_lite)], 100 * np.max(score_lite))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
